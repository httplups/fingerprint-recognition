{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb310563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "import math\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image \n",
    "#from flim.experiments import utils, LIDSDataset, ToTensor\n",
    "from os import path\n",
    "from math import ceil, floor\n",
    "from torch_snippets import * # Contains functions to manipulate images -- avoids to import cv2, glob, etc.\n",
    "\n",
    "#is GPU available?\n",
    "gpu = torch.cuda.is_available()\n",
    "\n",
    "#defining device where to to the computation\n",
    "device = torch.device(0) if gpu else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd68485",
   "metadata": {},
   "source": [
    "### Load dataset filenames and separate them into training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d97dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_perc  = 0.60\n",
    "valid_perc  = 0.20\n",
    "test_perc   = 0.20\n",
    "file_ext    = \"*.png\" \n",
    "base_dir    = path.join(\"images\", \"corel\") # base folder with dataset information\n",
    "fileset = Glob(path.join(base_dir, 'dataset/')+file_ext) # it returns a list of filenames\n",
    "num_train_samples = int(len(fileset)*train_perc)\n",
    "num_valid_samples = int(len(fileset)*valid_perc)  \n",
    "num_test_samples  = len(fileset) - num_train_samples - num_valid_samples \n",
    "from random import shuffle, seed; seed(10);\n",
    "shuffle(fileset)\n",
    "trainset = fileset[:num_train_samples]\n",
    "validset = fileset[num_train_samples:num_train_samples+num_valid_samples]\n",
    "testset  = fileset[num_train_samples+num_valid_samples:]\n",
    "nclasses    = 6\n",
    "model_name  = 'corel.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000e74c6",
   "metadata": {},
   "source": [
    "### Define preprocessing and a class that reads, transforms, and returns images and labels as tuples of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99415285",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision.transforms' has no attribute 'InterpolationMode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30536/2940805452.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m prep = transforms.Compose([\n\u001b[0;32m----> 5\u001b[0;31m     transforms.Resize((224,224), interpolation=transforms.InterpolationMode.BILINEAR, \n\u001b[0m\u001b[1;32m      6\u001b[0m                       max_size=None, antialias=True),\n\u001b[1;32m      7\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torchvision.transforms' has no attribute 'InterpolationMode'"
     ]
    }
   ],
   "source": [
    "# Regular preprocessing transformation. The input is a PIL image, which after being resized, \n",
    "# it is converted into a tensor for normalization using the ImageNet mean and stdev parameters. \n",
    "\n",
    "prep = transforms.Compose([\n",
    "    transforms.Resize((224,224), interpolation=transforms.InterpolationMode.BILINEAR, \n",
    "                      max_size=None, antialias=True),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))   \n",
    "])\n",
    "\n",
    "# Such transformations are applied everytime images are loaded from the filename lists in training, validation, \n",
    "# and test sets. We will do that during training, then by adding affine transformations and increasing the number \n",
    "# of epochs, we are actually implementing data augmentation. \n",
    "\n",
    "aug = transforms.Compose([\n",
    "    transforms.Resize((300,300), interpolation=transforms.InterpolationMode.BILINEAR, \n",
    "                      max_size=None, antialias=True),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.05,0.10), scale=(0.9,1.1), shear=(-2,2),\n",
    "                            interpolation=transforms.InterpolationMode.BILINEAR, \n",
    "                            fill=0, fillcolor=None, resample=None),\n",
    "    transforms.CenterCrop(250),\n",
    "    transforms.Resize((224,224), interpolation=transforms.InterpolationMode.BILINEAR, \n",
    "                      max_size=None, antialias=True),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))   \n",
    "])\n",
    "\n",
    "class DatasetImage(Dataset): # there are three mandatory functions: init, len, getitem\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        # it gets the image true labels and set the preprocessing transformation\n",
    "        self.dataset   = dataset\n",
    "        self.targets   = [int(str(x).split(\"/\")[-1].split(\"_\")[0])-1 for x in self.dataset]\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.dataset)        \n",
    "    def __getitem__(self, ix): # returns the item at position ix \n",
    "        filename = self.dataset[ix]\n",
    "        target   = self.targets[ix]\n",
    "        image    = Image.open(filename) # It is a PIL image\n",
    "        if (self.transform is not None):\n",
    "            image = self.transform(image)\n",
    "        else: # just reshape the image as a tensor with nchannels, height, width\n",
    "            image = torch.from_numpy(np.array(image)).permute(2,0,1).float()                  \n",
    "        return(image,target)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1126b81c",
   "metadata": {},
   "source": [
    "### Exemplify the execution of DatasetImage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cf3080",
   "metadata": {},
   "outputs": [],
   "source": [
    "datatensor = DatasetImage(trainset, aug)\n",
    "print(\"Number of images:\", len(datatensor))\n",
    "image, target = datatensor[80] # it executes getitem\n",
    "nchannels = image.shape[0]\n",
    "height    = image.shape[1]\n",
    "width     = image.shape[2]\n",
    "image     = image.permute(1,2,0).numpy()\n",
    "image     = 255*(image - np.min(image))/(np.max(image)-np.min(image))\n",
    "image     = image.astype('uint8')\n",
    "print(\"Images are {}x{}x{}\".format(width,height,nchannels))\n",
    "plt.imshow(image)\n",
    "print(\"Class of the image: \", target+1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a138df6",
   "metadata": {},
   "source": [
    "### Create functions that return random batches from the filename lists in the training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ca92c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 32\n",
    "\n",
    "def GetBatches(dataset, batchsize, transformation):\n",
    "    datatensor = DatasetImage(dataset, transformation) \n",
    "    dataloader = DataLoader(datatensor, batch_size=batchsize, shuffle=True)\n",
    "    return(dataloader)\n",
    "\n",
    "# as transformations, you may choose None, prep, or aug. However, aug applies to the training set only\n",
    "trainload = GetBatches(trainset, batchsize, aug)  \n",
    "validload = GetBatches(validset, batchsize, prep)\n",
    "testload  = GetBatches(testset, batchsize, prep) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aac906",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect(next(iter(trainload))) # inspect a couple of items in the batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0091688",
   "metadata": {},
   "source": [
    "### Define a ConvNet model compatible with the input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5641e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since you have used prep above, you have changed the input image size to 3 x 224 x 224\n",
    "nchannels   = 3\n",
    "height      = 224\n",
    "width       = 224\n",
    "input_shape = (nchannels, height, width)\n",
    "\n",
    "class CorelNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super(CorelNet, self).__init__()\n",
    "        \n",
    "        #defining feature extractor\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape[0], out_channels=32, kernel_size=(3, 3), \n",
    "                      stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(3, 3), padding=1, stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), \n",
    "                      stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(3, 3), padding=1, stride=2)\n",
    "        )\n",
    "\n",
    "        #defining classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=input_shape[1]//4*input_shape[2]//4*64, out_features=512, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.50),\n",
    "            nn.Linear(512, nclasses)\n",
    "        )\n",
    "\n",
    "        #initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "       #extracts features\n",
    "        x = self.features(x)\n",
    "\n",
    "        #transforms outputs into a 2D tensor\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        #classifies features\n",
    "        y = self.classifier(x)\n",
    "  \n",
    "        return y\n",
    "  \n",
    "    def _initialize_weights(self):\n",
    "        #for each submodule of our network\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                #get the number of elements in the layer weights\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels    \n",
    "                #initialize layer weights with random values generated from a normal\n",
    "                #distribution with mean = 0 and std = sqrt(2. / n))\n",
    "                m.weight.data.normal_(mean=0, std=math.sqrt(2. / n))\n",
    "\n",
    "                if m.bias is not None:\n",
    "                    #initialize bias with 0 \n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                #initialize layer weights with random values generated from a normal\n",
    "                #distribution with mean = 0 and std = 1/100\n",
    "                m.weight.data.normal_(mean=0, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                #initialize bias with 0 \n",
    "                    m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43755bd1",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71f7ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CorelNet(input_shape, nclasses).to(device)\n",
    "summary(model,input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ec4b6",
   "metadata": {},
   "source": [
    "### Train the model and visualize its training and validation loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5712f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining training loop\n",
    "def train(model, trainload, validload, criterion, optimizer, lr_scheduler, epochs, device):\n",
    "    train_loss_list = []\n",
    "    train_acc_list  = []\n",
    "    valid_loss_list = []\n",
    "    valid_acc_list  = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        #put the model in training mode\n",
    "        model.train() \n",
    "        \n",
    "        print('-' * batchsize)\n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0\n",
    "        n = 0\n",
    "\n",
    "        number_of_batches = len(trainload)\n",
    "        #run the model on a batch and compute mean loss and mean accuracy\n",
    "        for batch_index, data in enumerate(iter(trainload)):\n",
    "            inputs, labels = data\n",
    "\n",
    "            #mode data to the correct device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        \n",
    "            #zeroe gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #do forward through the neural network\n",
    "            outputs = model(inputs)\n",
    "    \n",
    "            #compute l2_regularization\n",
    "            \n",
    "            l2_regularization = 0\n",
    "            for param in model.parameters():\n",
    "                l2_regularization += torch.norm(param,2)\n",
    "                \n",
    "            \n",
    "            #compute the loss\n",
    "            loss = criterion(outputs, labels) + 0.001*l2_regularization\n",
    "            \n",
    "            #compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            #accumulate label prediction information to calculate accuracy\n",
    "            preds = torch.max(outputs, 1)[1]\n",
    "            running_loss     += loss.detach().cpu()*inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data).detach().cpu()\n",
    "            n += outputs.size(0)\n",
    "\n",
    "            print(\"\\rBatch {}/{} \".format(batch_index, number_of_batches), end=\"\")\n",
    "    \n",
    "        # compute and store mean loss and mean accuracy in their lists\n",
    "        \n",
    "        train_loss = running_loss/n\n",
    "        train_acc  = running_corrects.double()/n\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_acc_list.append(train_acc)\n",
    "        \n",
    "        # compute and store mean loss and mean accuracy on the validation set\n",
    "        valid_loss, valid_acc = test(model, validload, criterion, device)\n",
    "                              \n",
    "        valid_loss_list.append(valid_loss)\n",
    "        valid_acc_list.append(valid_acc)\n",
    "        \n",
    "        #count epochs\n",
    "        lr_scheduler.step()\n",
    "    \n",
    "        print('Training: Loss {:.6f}, Acc {:.6f} Validation: Loss {:.6f}, Acc {:.6f}'.format(train_loss, \\\n",
    "                                                                                             train_acc, \\\n",
    "                                                                                             valid_loss, \\\n",
    "                                                                                             valid_acc))\n",
    "        \n",
    "    return(train_loss_list, train_acc_list, valid_loss_list, valid_acc_list)  \n",
    "\n",
    "def test(model, dataload, criterion, device):\n",
    "    #put model in evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0\n",
    "        n = 0\n",
    "    \n",
    "        number_of_batches = len(dataload)\n",
    "  \n",
    "        #process all batches without gradients computation\n",
    "        for batch_index, data in enumerate(iter(dataload)):\n",
    "            inputs, labels = data\n",
    "\n",
    "            #mode data to the correct device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "             \n",
    "            #do forward through model\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            #calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "      \n",
    "            #accumulate information to calculate mean loss and mean accuracy\n",
    "            preds             = torch.max(outputs, 1)[1]\n",
    "            running_loss     += loss.detach().cpu()*inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data).detach().cpu()\n",
    "            n                += outputs.size(0)\n",
    "        \n",
    "            data_loss = running_loss/n\n",
    "            data_acc  = running_corrects.double()/n\n",
    "    \n",
    "    return(data_loss, data_acc)   \n",
    "\n",
    "# Define criterion and optimizer\n",
    "\n",
    "criterion    = nn.CrossEntropyLoss()\n",
    "optimizer    = optim.Adam(model.parameters(), lr=1e-5)\n",
    "lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.99) \n",
    "#lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f071652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "#del trainset, validset, testset\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "nepochs = 25\n",
    "history = train(model, trainload, validload, criterion, optimizer, lr_scheduler, nepochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf81760",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c4d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [ i for i in range(1, len(history[0])+1)]\n",
    "plt.plot(epochs, history[0], 'bo', label='Training loss')\n",
    "plt.plot(epochs, history[2], 'b', label='Validation loss')\n",
    "plt.title('Training and Validation Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss Values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.clf\n",
    "epochs = [ i for i in range(1, len(history[1])+1)]\n",
    "plt.plot(epochs, history[1], 'bo', label='Training acc')\n",
    "plt.plot(epochs, history[3], 'b', label='Validation acc')\n",
    "plt.title('Training and Validation Acc Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515c7457",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c0b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "(loss, acc) = test(model, testload, criterion, device)\n",
    "print('Loss: {:.6f} Acc: {:.6f}'.format(loss.to(device),acc.to(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864b4f36",
   "metadata": {},
   "source": [
    "### Visualize activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8906bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a1df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to return activations of all layers. It depends on the model. \n",
    "\n",
    "def get_output_by_layer(model, x):\n",
    "    #empty dict\n",
    "    output_by_layer = OrderedDict()\n",
    "  \n",
    "    #get the input\n",
    "    output_by_layer['input'] = x.clone().detach().cpu().data.numpy()\n",
    "\n",
    "    #for each layer of the feature extractor\n",
    "    for layer_name, layer in model.features.named_children():\n",
    "        #do forward through the layer\n",
    "        x = layer.forward(x)\n",
    "        #save the output\n",
    "        output_by_layer[\"features-\"+layer_name] = x.clone().detach().cpu().numpy()\n",
    "  \n",
    "\n",
    "\n",
    "    #transform features to a 2D tensor: you do not need to show this one\n",
    "    x = x.flatten(start_dim=1)\n",
    "    \n",
    "    #for each layer of the classifier\n",
    "    for layer_name, layer in model.classifier.named_children():\n",
    "        #do forward through the layer   \n",
    "        x = layer.forward(x)\n",
    "        #save the output\n",
    "        output_by_layer[\"classifier-\"+layer_name] = x.clone().detach().cpu().numpy()\n",
    "  \n",
    "    #return output by layer\n",
    "    return output_by_layer\n",
    "\n",
    "#get the output activations from all layers and labels\n",
    "def get_ouputs(model, dataload, device, model_name):\n",
    "    outputs_by_layer = None\n",
    "    all_labels = None\n",
    "\n",
    "    #get a batch from the dataload\n",
    "    for inputs, labels in dataload:\n",
    "        #move inputs to the correct device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.clone().detach().cpu().numpy()\n",
    "        \n",
    "        #get outputs by layer\n",
    "        outputs = get_output_by_layer(model, inputs)\n",
    "            \n",
    "        #save the outputs\n",
    "        if outputs_by_layer is None:\n",
    "            outputs_by_layer = outputs\n",
    "            all_labels       = labels\n",
    "        else:\n",
    "            for layer in outputs:\n",
    "                outputs_by_layer[layer] = np.concatenate((outputs_by_layer[layer], outputs[layer]), axis=0)\n",
    "            all_labels = np.concatenate((all_labels, labels))   \n",
    "\n",
    "    return outputs_by_layer, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50010ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import umap to calculate projection\n",
    "import umap\n",
    "#import tsne to calculate projection\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# select a projection algorithm\n",
    "reducer = umap.UMAP()\n",
    "#reducer = TSNE(perplexity=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd5886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#maps from high dimension to 2D\n",
    "def projection(outputs_by_layer, reducer):\n",
    "    projection_by_layer = OrderedDict()\n",
    "\n",
    "    for layer in outputs_by_layer:\n",
    "        #get the output of layer\n",
    "        output = outputs_by_layer[layer]\n",
    "        output = output.reshape(output.shape[0], -1)\n",
    "        #map to 2D\n",
    "        embedded = reducer.fit_transform(output)\n",
    "        #save projection\n",
    "        projection_by_layer[layer] = embedded\n",
    "  \n",
    "    return projection_by_layer\n",
    "\n",
    "#plot the projection of the output of each layer\n",
    "def create_visualization(projection_by_layer, all_labels):\n",
    "  \n",
    "    for layer in projection_by_layer:\n",
    "        embedded = projection_by_layer[layer]\n",
    "  \n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        plt.scatter(embedded[:, 0], embedded[:, 1], c=all_labels, cmap=plt.get_cmap('tab10'))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(layer)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a40fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_by_layer, all_labels = get_ouputs(model, trainload, device, model_name)\n",
    "projection_by_layer          = projection(outputs_by_layer, reducer)\n",
    "create_visualization(projection_by_layer, all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a085a5b",
   "metadata": {},
   "source": [
    "### Visualize attention to explain a decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da1e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get activations at the output of the feature extractor\n",
    "def get_activations(model, x, device):\n",
    "    # move input tensor x to the selected device\n",
    "    x = x.to(device)\n",
    "    # get activations after feature extraction\n",
    "    x = model.features(x)\n",
    "    \n",
    "    #return output by layer\n",
    "    return x\n",
    "\n",
    "# Define a function to get the output of the model\n",
    "def get_output_of_the_model(model, x, device):\n",
    "    # put the model in the evaluation mode\n",
    "    model.eval() \n",
    "    # move input tensor x to the selected device\n",
    "    x = x.to(device) \n",
    "    # execute the model with gradients\n",
    "    output = model(x)\n",
    "    return(output)\n",
    "\n",
    "def get_heatmap(model, x, device, model_name):\n",
    "    # add one dimension (dim=0 must be the number of images)\n",
    "    xin = x.unsqueeze(0)\n",
    "    \n",
    "    # get the output of the feature extractor \n",
    "    activ = get_activations(model, xin, device)\n",
    "\n",
    "    # get the predictions at the output of the decision layer\n",
    "    logits = get_output_of_the_model(model, xin, device)\n",
    "    \n",
    "    # get the most confident prediction\n",
    "    pred   = logits.max(-1)[-1] \n",
    "    print(\"predicted label: \", pred.cpu().detach().numpy()[0]+1)\n",
    "    \n",
    "    # compute gradients with respect to the most confident prediction\n",
    "    model.zero_grad() \n",
    "    logits[0,pred].backward(retain_graph=True) \n",
    "    \n",
    "    # get the number of channels\n",
    "    nchannels = activ.shape[1] \n",
    "\n",
    "    # only conv2D generates gradients, use the last one \n",
    "    conv_layername = '4'\n",
    "    children       = model.features.named_children()\n",
    "    \n",
    "    for layer_name, layer in children:\n",
    "        if layer_name == conv_layername:\n",
    "            grad = layer.weight.grad\n",
    "            # 64 filters of size 3x3 x 32 channels\n",
    "            print(\"Shape of the gradient tensor: \", grad.shape)     \n",
    "    \n",
    "            # compute the weighted mean of the activations across channels using the mean value of the gradient \n",
    "            # of each filter parameter as weight. The result is a heatmap.\n",
    "            for i in range(nchannels):\n",
    "                activ[:,i,:,:] *= grad[i].mean()\n",
    "            heatmap = torch.mean(activ, dim=1)[0].cpu().detach()\n",
    "\n",
    "            # convert to numpy, normalize, and resize to the input size\n",
    "            heatmap = heatmap.squeeze(0).numpy()\n",
    "            heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "            heatmap = cv2.resize(heatmap, (x.shape[2], x.shape[1]))\n",
    "    \n",
    "            return(heatmap)\n",
    "    \n",
    "    return(None)\n",
    "\n",
    "def display_image_with_heatmap(img, heatmap, scale):\n",
    "    heatmap = np.uint8(255.0*heatmap)\n",
    "    width   = int(heatmap.shape[1]*scale)\n",
    "    height  = int(heatmap.shape[0]*scale)\n",
    "    heatmap = cv2.resize(heatmap, (width, height))\n",
    "    img     = cv2.resize(img, (width, height))\n",
    "    heatmap = cv2.applyColorMap(255-heatmap, cv2.COLORMAP_JET)\n",
    "    heatmap = np.uint8(heatmap)\n",
    "    heatmap = np.uint8(heatmap*0.7 + img*0.3)\n",
    "    plt.imshow(heatmap)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Show CAM for a given image\n",
    "datatensor = DatasetImage(trainset, prep)\n",
    "image, target = datatensor[80]\n",
    "\n",
    "heatmap       = get_heatmap(model, image, device, model_name)\n",
    "image         = image.permute(1,2,0).numpy()\n",
    "image         = 255*(image - np.min(image))/(np.max(image)-np.min(image))\n",
    "image         = image.astype('uint8')\n",
    "print(\"true label: \", target+1)\n",
    "\n",
    "if (heatmap is not None):\n",
    "    display_image_with_heatmap(image, heatmap, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29fdb74",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e40cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.to('cpu').state_dict(), model_name)\n",
    "#model.to(device) # return model to device in case you want to execute previous cells"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
